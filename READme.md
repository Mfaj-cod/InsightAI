> InsightAI :â€“ Retrieval-Augmented Generation (RAG) App

    InsightAI is a Retrieval-Augmented Generation (RAG) web application built with Flask (backend) and HTML + Bootstrap (frontend).
        It enables you to upload documents/images, extract text using OCR, create embeddings, store them in a vector database, and query the knowledge base with the help of LLMs like Ollamaâ€™s open-source models.

> Features:-

    ğŸ–¼ Upload documents/images (PNG, JPG, JPEG, PDF)

    ğŸ” OCR text extraction (Tesseract and Google Vision API)

    âœ‚ Text chunking for better retrieval

    ğŸ§  Embeddings generation (via HuggingFace/Sentence-Transformers or OpenAI/Ollama)

    ğŸ“¦ Vector database support (FAISS)

    ğŸ’¾ SQLAlchemy integration for document metadata storage

    ğŸ’¬ Chat with your documents using RAG pipeline and LLM (Ollama by default and OpenAI on fallback)

    ğŸŒ Frontend built with Flask templates (Bootstrap, JS, CSS)


> Project Structure:-

InsightAI/
â”‚â”€â”€ app.py                # Flask app entry point
â”‚â”€â”€ config.py             # Configuration settings
â”‚â”€â”€ requirements.txt      # Dependencies
â”‚â”€â”€ README.md             # Project documentation
â”‚
â”œâ”€â”€ modules/              
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ocr.py            # OCR functions
â”‚   â”œâ”€â”€ chunking.py       # Text chunking
â”‚   â”œâ”€â”€ embeddings.py     # Embedding generation
â”‚   â”œâ”€â”€ vectorstore.py    # Vector DB operations
â”‚   â”œâ”€â”€ rag_pipeline.py   # RAG pipeline logic
â”‚   â””â”€â”€ db.py             # SQLalchemy DB models
â”‚
â”œâ”€â”€ templates/           
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ docs.html
â”‚   â””â”€â”€ chat.html
â”‚
â”œâ”€â”€ static/               
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js
â”‚   â””â”€â”€ uploads/          # Uploaded files
â”‚
â””â”€â”€ tests/                
    â”œâ”€â”€ test_ocr.py
    â”œâ”€â”€ test_embeddings.py
    â”œâ”€â”€ test_vectorstore.py
    â””â”€â”€ test_rag_pipeline.py


> Installation:-

    1. Clone the repository
    git clone https://github.com/yourusername/InsightAI.git
    cd InsightAI

    2. Create a virtual environment
    python -m venv venv
    source venv/bin/activate      # Linux/Mac
    venv\Scripts\activate         # Windows

    3. Install dependencies
    pip install --upgrade pip
    pip install -r requirements.txt

    4. (Optional) Install FAISS for vector search
    pip install faiss-cpu

    5. Install and run Ollama (if using local models)

    Download Ollama

    Verify installation:

    ollama run llama2

    â–¶ï¸ Running the App
    python app.py


    The app will be available at http://127.0.0.1:5000/

> Usage:-

    Open the web app in your browser.

    Upload a PDF/image document.

    The system will extract text, chunk it, and store embeddings.

    Go to Chat and ask questions related to your uploaded documents.


> Future Improvements:-

    Add support for multi-document queries

    Integrate LangChain for pipeline flexibility

    UI enhancements (dark mode, history view)

    Dockerize the app for deployment

    Add cloud vector DBs (Pinecone, Weaviate, Qdrant)

> License:-

    This project is licensed under the MIT License â€“ feel free to use and modify it.


> System design:-


                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       User (Web)        â”‚
                â”‚  â€¢ Upload document/img  â”‚
                â”‚  â€¢ Ask questions (chat) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Flask Web Server    â”‚
                â”‚(Routes + Business Logic)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OCR Engine   â”‚  â”‚ Vector Databaseâ”‚  â”‚    LLM API         â”‚
â”‚ (Tesseract /   â”‚  â”‚ (Qdrant / Pine-â”‚  â”‚ (OpenAI GPT /      â”‚
â”‚ Google Vision) â”‚  â”‚cone / Weaviate)â”‚  â”‚ LLaMA local model) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â–²                   â–²
        â–¼                   â”‚                   â”‚
  Extracted Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Embeddings â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Document Store (DB)   â”‚
                â”‚ Metadata + Original Docsâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ”¹ Flow Explanation

Upload Document (Web Frontend)

    User uploads PDF/image via Bootstrap form.

    Flask handles the file and sends it to OCR.

OCR & Preprocessing

    OCR extracts raw text.

    Flask cleans and chunks text.

Embedding + Storage

    Each chunk is embedded (OpenAI embeddings or SentenceTransformers).

    Stored in a vector DB (FAISS locally).

    Also save metadata in a relational DB (SQLAlchemy) for tracking.

Query Phase

    User types a question.

    Flask generates embedding of the query.

    Vector DB retrieves top-k similar chunks.

LLM Answering

    Retrieved chunks + user question sent to LLM (OpenAI API or local).

    LLM generates context-aware answer.

    Response Display

    Flask returns the answer to frontend.

    Bootstrap renders it in a chat-style UI.



