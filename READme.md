> InsightAI :â€“ Retrieval-Augmented Generation (RAG) App

    InsightAI is a Retrieval-Augmented Generation (RAG) web application built with Flask (backend) and HTML + Bootstrap (frontend).
        It enables you to upload documents/images, extract text using OCR, create embeddings, store them in a vector database, and query the knowledge base with the help of LLMs like Ollamaâ€™s open-source models.

> Features:-

    ğŸ–¼ Upload documents/images (PNG, JPG, JPEG, PDF)

    ğŸ” OCR text extraction (Tesseract and Google Vision API)

    âœ‚ Text chunking for better retrieval

    ğŸ§  Embeddings generation (via HuggingFace/Sentence-Transformers or OpenAI/Ollama)

    ğŸ“¦ Vector database support (FAISS)

    ğŸ’¾ SQLAlchemy integration for document metadata storage

    ğŸ’¬ Chat with your documents using RAG pipeline and LLM (Ollama by default and OpenAI on fallback)

    ğŸŒ Frontend built with Flask templates (Bootstrap, JS, CSS)



> Installation:-

    1. Clone the repository
    git clone https://github.com/yourusername/InsightAI.git
    cd InsightAI

    2. Create a virtual environment
    python -m venv venv
    source venv/bin/activate      # Linux/Mac
    venv\Scripts\activate         # Windows

    3. Install dependencies
    pip install --upgrade pip
    pip install -r requirements.txt

    4. (Optional) Install FAISS for vector search
    pip install faiss-cpu

    5. Install and run Ollama (if using local models)

    Download Ollama

    Verify installation:

    ollama run llama2

    â–¶ï¸ Running the App
    python app.py


    The app will be available at http://127.0.0.1:5000/

> Usage:-

    Open the web app in your browser.

    Upload a PDF/image document.

    The system will extract text, chunk it, and store embeddings.

    Go to Chat and ask questions related to your uploaded documents.


> Future Improvements:-

    Add support for multi-document queries

    Integrate LangChain for pipeline flexibility

    UI enhancements (dark mode, history view)

    Dockerize the app for deployment

    Add cloud vector DBs (Pinecone, Weaviate, Qdrant)

> License:-

    This project is licensed under the MIT License â€“ feel free to use and modify it.


System design:-


                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       User (Web)        â”‚
                â”‚  â€¢ Upload document/img  â”‚
                â”‚  â€¢ Ask questions (chat) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Flask Web Server    â”‚
                â”‚(Routes + Business Logic)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OCR Engine   â”‚  â”‚ Vector Databaseâ”‚  â”‚    LLM API         â”‚
â”‚ (Tesseract /   â”‚  â”‚ (Qdrant / Pine-â”‚  â”‚ (OpenAI GPT /      â”‚
â”‚ Google Vision) â”‚  â”‚cone / Weaviate)â”‚  â”‚ LLaMA local model) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â–²                   â–²
        â–¼                   â”‚                   â”‚
  Extracted Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Embeddings â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Document Store (DB)   â”‚
                â”‚ Metadata + Original Docsâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ”¹ Flow Explanation

Upload Document (Web Frontend)

    User uploads PDF/image via Bootstrap form.

    Flask handles the file and sends it to OCR.

OCR & Preprocessing

    OCR extracts raw text.

    Flask cleans and chunks text.

Embedding + Storage

    Each chunk is embedded (OpenAI embeddings or SentenceTransformers).

    Stored in a vector DB (FAISS locally).

    Also save metadata in a relational DB (SQLAlchemy) for tracking.

Query Phase

    User types a question.

    Flask generates embedding of the query.

    Vector DB retrieves top-k similar chunks.

LLM Answering

    Retrieved chunks + user question sent to LLM (OpenAI API or local).

    LLM generates context-aware answer.

    Response Display

    Flask returns the answer to frontend.

    Bootstrap renders it in a chat-style UI.



